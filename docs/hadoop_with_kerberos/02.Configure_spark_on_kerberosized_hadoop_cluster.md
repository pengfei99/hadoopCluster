# Configure spark on a hadoop cluster with kerberos

Suppose the Hadoop cluster has the below architecture:

```text
10.50.5.199	    hadoop-client.casdds.casd	# hadoop-client
10.50.5.203	    spark-m01.casdds.casd	# name node
10.50.5.204     spark-m02.casdds.casd   # data node 1
10.50.5.205     spark-m03.casdds.casd  # data node 2
```

Our goal is to install and configure a `spark client` in `hadoop-client`, which can submit spark jobs to the hadoop
cluster. 

## Configure your hadoop client

In our case, spark use yarn to submit job on the cluster, so we need to make sure the configure of hadoop client is correct.
You will need to edit three configuration files in `/opt/hadoop/hadoop-3.3.6/etc/hadoop`:
- core-site.xml
- hdfs-site.xml
- yarn-site.xml

> For more details, you can check [Configure_hadoop_client_with_kerberos.md](./01.Configure_hadoop_client_with_kerberos.md)
> 

## Configure your spark default

```shell
# create a custom spark client conf folder
mkdir ~/.spark/conf
vim  ~/.spark/conf/spark-defaults.conf

# add the below content
spark.hadoop.fs.defaultFS hdfs://spark-m01.casdds.casd:8020
spark.hadoop.hadoop.security.authentication kerberos
spark.hadoop.hadoop.security.authorization true
spark.hadoop.dfs.encrypt.data.transfer true
spark.hadoop.dfs.data.transfer.protection authentication

spark.yarn.security.credentials.hadoopfs.enabled   true
spark.yarn.security.tokens.hadoopfs.enabled    true
spark.hadoop.hadoop.security.group.mapping.ldap.bind.user  ldap-user
spark.hadoop.hadoop.security.group.mapping  org.apache.hadoop.security.LdapGroupsMapping


# spark.kerberos.principal               pliu@CASDDS.CASD
# spark.kerberos.keytab                  /home/pliu/pliu-user.keytab
spark.yarn.security.credentials.hadoop.enabled true

spark.driver.memory                512M
spark.executor.memory              512M
spark.executor.instances           2

```

Add the spark-default to your env var

```shell
vim ~/.bashrc

# add the below content at the end of the file
export SPARK_CONF_DIR=$HOME/.spark/conf
```

## Submit spark jobs to hadoop cluster

Spark can take kerberos tickets automatically to the hadoop cluster via yarn.
We don't need to specific configuration. We only need to make sure the user
has obtained a valid TGT ticket.

But if the spark job runs longer than the validity of the ticket, the job will fail. So the best practice
is to use a keytab file which allows spark to ask new tickets if it needs. Below is an example
of the spark-submit commands

```shell
spark-submit --master yarn --deploy-mode cluster \
  --principal hadoop-user@EXAMPLE.COM \
  --keytab /etc/security/keytabs/hadoop-user.keytab \
  --class com.example.MyApp hdfs:///user/hadoop-user/myapp.jar
```

You can also configure the `spark-defaults.conf`:

```shell
spark.yarn.principal hadoop-user@EXAMPLE.COM
spark.yarn.keytab /etc/security/keytabs/hadoop-user.keytab
spark.hadoop.fs.defaultFS hdfs://namenode1.example.com:9000
```


## Test your cluster

### Test 1 : Valid the spark kerberos integration

In test 1, we only do some simple calculation, without accessing hdfs

```python
from pyspark.sql import SparkSession

def main():
    # Create Spark session
    spark = SparkSession.builder \
        .appName("KerberosSparkTest") \
        .getOrCreate()

    # For a basic test, create a small DataFrame
    df = spark.createDataFrame([
        ("Alice", 25),
        ("Bob", 30),
        ("Charlie", 35)
    ], ["name", "age"])

    df.show()

    # Just for validation: print row count
    print(f"Total rows: {df.count()}")

    spark.stop()

if __name__ == "__main__":
    main()

```
```shell
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --conf spark.hadoop.hadoop.security.authentication=kerberos \
  job1.py
```




```shell
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --conf spark.hadoop.hadoop.security.authentication=kerberos \
  --conf spark.hadoop.yarn.resourcemanager.address=spark-m01.casdds.casd:8032 \
  --principal pliu@CASDDS.CASD \
  --keytab /home/pliu/pliu-user.keytab \
  job1.py
```


### Test 2 Spark delegate kerberos ticket for Hdfs access control

In test2, we will read a csv file from hdfs, filter users who work for the federal government, then we group workers by sex and 
count the worker number. We will also write the result in hdfs.


```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

def main():
    # Create Spark session
    spark = SparkSession.builder \
        .appName("census_stats") \
        .getOrCreate()

    # read a file in hdfs
    us_censure_path = "hdfs:///user/pengfei/us_census_1996.csv"
    result_path = "hdfs:///user/pengfei/tmp/census_stats"
    df = spark.read \
        .options(header=True, inferSchema=True, delimiter=',', nullValue="?") \
        .csv(path=us_censure_path)
    df.show(5)
    result = df.filter(col("workclass")=="State-gov").groupby("sex").count()

    # Just for validation: print row count
    print(f"Total rows: {df.count()}")
    result.write.mode("overwrite").options(header=True, delimiter=",").csv(result_path)

    spark.stop()

if __name__ == "__main__":
    main()
```

```shell
spark-submit   --master yarn   --deploy-mode cluster job2.py
```


### Test 3: Test long running spark jobs

In this test, we changed the Ad/Krb ticket policy, the TGT validity is `1 hour`, renewable for `1 day`. The below job
will take about 20 hours to finish. If the job finishes correctly, it means the hadoop cluster can automatically renew
the ticket. 

```shell
from pyspark.sql import SparkSession
import time
def main():
    # Create Spark session
    spark = SparkSession.builder \
        .appName("pengfei_long_running_jobs") \
        .getOrCreate()
    ite_num = 120
    i = 0
    # each iteration sleep for 10 mins
    while i< ite_num:
        # do a basic df count
        df = spark.createDataFrame([
            ("Alice", 25),
            ("Bob", 30),
            ("Charlie", 35)
        ], ["name", "age"])
        df.count()
        # sleep 10 mins
        time.sleep(600)
        i +=1
        print(f"Sleep Iteration: {i}")
    
    spark.stop()

if __name__ == "__main__":
    main()

```