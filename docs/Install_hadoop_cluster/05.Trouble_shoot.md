# Trouble shoot 

## 1. Permission denied with user=dr.who

The default static username when you user hdfs web ui is called **dr.who**. But when a job spark-submit write on the
hdfs uses the username of the os (e.g. linux, windows) with the default acls. As a result, when user access the data
via web ui with username `dr.who`, you will receive the **permission denied** error.

### 1.1 Simple solution

we can change the default static username to `hadoop` which is considered as the default root user. So he has right to
access any user file.

In `core-site.xml`, add the following lines

```xml
<property>
	<name>hadoop.http.staticuser.user</name>
	<value>hadoop</value>
	<description>The user name to filter as, on static web filters while rendering content. An example use 
	is the HDFS web UI (user to be used for browsing files).</description>
</property>
```

If this value is empty, `dr.who` will be used.

The disadvantage is that anyone who can access the web ui can delete all files

http://10.50.6.103:9870/


