# Adding and deleting new data nodes

## 1. Adding new datanode by using white list

It's recommended to add new datanode by using white list. This can avoid some hacker attack

### 1.1 Configure a white list for the cluster

Step1: Create a white list

```shell
# go to the hadoop config folder
cd /path/to/hadoop/etc/hadoop

# create a white list file
vim whitelist

# add the hostname of the datanode which you want to add. for example
dn-11
dn-12
dn-13
```

Step2: In `hdfs-site.xml`, add the white list as trusted hosts

```xml
<property>
	<name>dfs.hosts</name>
	<value>/path/to/hadoop/etc/hadoop/whitelist</value>
	<description>Names a file that contains a list of hosts that are permitted to connect to the namenode. 
        The full pathname of the file must be specified. If the value is empty, all hosts are permitted.</description>
</property>

```

Step3: copy the `whitelist` and `hdfs-site.xml` to all nodes in the cluster.

> You need to restart the cluster for the first time when you edit the hdfs-site.xml. After that you only need to
> edit the whitelist and run the refreshNode command

```shell
# command to refresh the node list in the cluster
hdfs dfsadmin -refreshNode
```

### 1.2 Prepare the datanode

After 1.1, the cluster knows that there are new datanodes, you still need to start the service to enable the communication
between namenode and datanodes.

Step1: Add the `jdk, hadoop and config file` to the new datanodes

Step2: run below command to start the service

```shell
# start the dfs daemon
hdfs --daemon start datanode

# start the yarn daemon
yarn --daemon start nodemanager
```


## 2. Deleting(decommissioning) a datanode from the cluster

The best way for Deleting/decommissioning a datanode is to use blacklist option. It tells the cluster, the datanodes
in the blacklist are no longer able to store data blocks, hdfs will duplicate data blocks which are in the blacklist
datanodes to other available datanodes.

### 2.1 Configure a blacklist

Step1: Create a `blacklist` file

```shell
# go to the hadoop config folder
cd /path/to/hadoop/etc/hadoop

# create a white list file
vim blacklist

# add the hostname of the datanode which you want to delete. for example
bad-dn-16
bad-dn-17
bad-dn-18
```

Step2: In `hdfs-site.xml`, add the white list as trusted hosts

```xml
<property>
	<name>dfs.hosts.exclude</name>
	<value>/path/to/hadoop/etc/hadoop/blacklist</value>
	<description>Names a file that contains a list of hosts that are not permitted to connect to the namenode. 
        The full pathname of the file must be specified. If the value is empty, no hosts are excluded.</description>
</property>

```

Step3: copy the `blacklist` and `hdfs-site.xml` to all nodes in the cluster.

> You need to restart the cluster for the first time when you edit the hdfs-site.xml. After that you only need to
> edit the blacklist and run the refreshNode command

```shell
# command to refresh the node list in the cluster
hdfs dfsadmin -refreshNode
```

### 2.2 Stop the deleting datanodes

After you restart the cluster, you should notice the status of the blacklisted datanodes are changed (i.e. `In service` -> `Decommissioning`).
**Do not stop the datanode in decommissiong state**, because hdfs is copying all the data block to other datanodes.

You should wait their status to change from `Decommissioning` to `Decommissioned`. Once they are in `Decommissioned` status,
it means all data blocks have been copied, it's safe to stop the datanode now. You can run the below command to stop
the hadoop daemon
```shell
hdfs --daemon stop datanode
hdfs --daemon stop nodemanage
```

> If you config hdfs to have 3 replicat for each data block, and you only have 2 available datanodes in the cluster, 
> the blacklisted datanodes will never enter the `Decommissioned` status. Because the copy process will never finish. 



## 3 Rebalance data blocks between datanodes

After adding/deleting a data node, you will find there is 0 data block on the new datanodes. Run the below command to rebalance
the data blocks.

```shell
# the number of threshold means after rebalance, the difference between disk usage between datanodes should be less than 10% 
/path/to/hadoop/sbin/start-balancer.sh -threshold 10

# you can stop the balancer
/path/to/hadoop/sbin/stop-balancer.sh
```

> hdfs will start a new **rebalance server** to do the balancing. So don't run it on the namenode, if your namenode is 
> already busy. If you don't stop the **rebalance server**, it will run as a daemon forever.