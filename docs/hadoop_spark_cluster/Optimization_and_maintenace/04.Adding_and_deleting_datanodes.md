# Adding and deleting new data nodes

## 1. Adding new datanode by using white list

It's recommended to add new datanode by using white list. This can avoid some hacker attack

### 1.1 Configure a white list for the cluster

Step1: Create a white list

```shell
# go to the hadoop config folder
cd /path/to/hadoop/etc/hadoop

# create a white list file
vim whitelist

# add the hostname of the datanode which you want to add. for example
dn-11
dn-12
dn-13
```

Step2: In `hdfs-site.xml`, add the white list as trusted hosts

```xml
<property>
	<name>dfs.hosts</name>
	<value>/path/to/hadoop/etc/hadoop/whitelist</value>
	<description>Names a file that contains a list of hosts that are permitted to connect to the namenode. 
        The full pathname of the file must be specified. If the value is empty, all hosts are permitted.</description>
</property>

```

Step3: copy the `whitelist` and `hdfs-site.xml` to all nodes in the cluster.

> You need to restart the cluster for the first time when you edit the hdfs-site.xml. After that you only need to
> edit the whitelist and run the refreshNodes command

```shell
# command to refresh the node list in the cluster
hdfs dfsadmin -refreshNodes
```

### 1.2 Prepare the datanode

After 1.1, the cluster knows that there are new datanodes, you still need to start the service to enable the communication
between namenode and datanodes.

Step1: Add the `jdk, hadoop and config file` to the new datanodes

Step2: run below command to start the service

```shell
# start the dfs daemon
hdfs --daemon start datanode

# start the yarn daemon
yarn --daemon start nodemanager
```


## 2. Deleting(decommissioning) a datanode from the cluster

The best way for Deleting/decommissioning a datanode is to use blacklist option. It tells the cluster, the datanodes
in the blacklist are no longer able to store data blocks, hdfs will duplicate data blocks which are in the blacklist
datanodes to other available datanodes.

### 2.1 Configure a blacklist

Step1: Create a `blacklist` file

```shell
# go to the hadoop config folder
cd /path/to/hadoop/etc/hadoop

# create a white list file
vim blacklist

# add the hostname of the datanode which you want to delete. for example
bad-dn-16
bad-dn-17
bad-dn-18
```

Step2: In `hdfs-site.xml`, add the white list as trusted hosts

```xml
<property>
	<name>dfs.hosts.exclude</name>
	<value>/path/to/hadoop/etc/hadoop/blacklist</value>
	<description>Names a file that contains a list of hosts that are not permitted to connect to the namenode. 
        The full pathname of the file must be specified. If the value is empty, no hosts are excluded.</description>
</property>

```

Step3: copy the `blacklist` and `hdfs-site.xml` to all nodes in the cluster.

> You need to restart the cluster for the first time when you edit the hdfs-site.xml. After that you only need to
> edit the blacklist and run the refreshNodes command

```shell
# command to refresh the node list in the cluster
hdfs dfsadmin -refreshNodes
```

### 2.2 Stop the deleting datanodes

After you restart the cluster, you should notice the status of the blacklisted datanodes are changed (i.e. `In service` -> `Decommissioning`).
**Do not stop the datanode in decommissiong state**, because hdfs is copying all the data block to other datanodes.

You should wait their status to change from `Decommissioning` to `Decommissioned`. Once they are in `Decommissioned` status,
it means all data blocks have been copied, it's safe to stop the datanode now. You can run the below command to stop
the hadoop daemon
```shell
hdfs --daemon stop datanode
hdfs --daemon stop nodemanage
```

> If you config hdfs to have 3 replicat for each data block, and you only have 2 available datanodes in the cluster, 
> the blacklisted datanodes will never enter the `Decommissioned` status. Because the copy process will never finish. 



## 3 Rebalance data blocks between datanodes

After adding/deleting a data node, you will find there is 0 data block on the new datanodes. Run the below command to rebalance
the data blocks.

```shell
# the number of threshold means after rebalance, the difference between disk usage between datanodes should be less than 10% 
/path/to/hadoop/sbin/start-balancer.sh -threshold 10

# you can stop the balancer
/path/to/hadoop/sbin/stop-balancer.sh
```

> hdfs will start a new **rebalance server** to do the balancing. So don't run it on the namenode, if your namenode is 
> already busy. If you don't stop the **rebalance server**, it will run as a daemon forever.


## 4. Control the data blocks replication speed

When we decommission datanodes or rebalance, hdfs will copy data blocks between data nodes.

To increase the speed of the replicat copy, we can modify the below three params in **hdfs-site.xml**

> The copy process will consume resources too, if hdfs use too much resource to do internal copy, the performence for the 
> hdfs client may be impacted. So find a good compromise between the internal copy speed and the client action speed.

### 4.1. For Namenode: Change the replication iteration number

**dfs.namenode.replication.work.multiplier.per.iteration**: it defines the block number which a data node can duplicate
during one heartbeat. The default value is 2. By increasing this number, the copy speed increase too. But the datanode
process will consume more resource too.

For example, if a cluster has 500 nodes, and the iteration=10. It means for one heartbeat, the namenode can send a list
of 10*500=5000 block. If we decommission one node which contains 800000 block, it will take 800000/(500*10)=160 heartbeat
if one heartbeat takes every 3 sec. The total time will be 160*3 = 480 seconds.

> If the datanodes is fixed in a cluster, increase the iteration number increase the speed.

For our cluster, we set the value to 5.

### 4.2. For Datanode: 

Block copy priorities:
1. L1(highest): when a block has 0 duplicates in the cluster, the block has the possibility to be lost(no failure). So the block copy has the highest priority.
2. L2: when a block has minimum duplicates(less than 1/3 of the expected duplication number), 
3. L3: has better block duplicat number than L2
4. L4: block has the minimum duplication number.
5. L5: the block has error, but it has enough duplicats works normally

We need to modify the below two params  
1. **dfs.namenode.replication.max-streams**: It defines the max thread number which will be used to copy the block inside the datanode.
   The block copy process has 5 different type of priorities, the L1 copy process are not limited by this configuration.
   The default value is 2. we recommend 10 

2. **dfs.namenode.replication.max-streams-hard-limit**: It defines the max thread number which will be used to copy the block which
   has the priority L1. The default value is 4, we recommend 20.
