# Yarn introduction

**YARN**(Yet Another Resource Negotiator), was introduced in Hadoop version 2.0 by Yahoo and Hortonworks in 2012. 
The basic idea of YARN in Hadoop is to divide the functions of resource management and task scheduling/monitoring 
into separate daemon processes.

YARN in Hadoop allows for the execution of various data processing engines such as batch processing, graph processing, 
stream processing, and interactive processing, as well as the processing of data stored in HDFS.

> If you consider yarn as a Operating System for a distributed calculation cluster, then the jobs (MapReduce, spark, etc.)
> are the applications run on the OS.

## Why Yarn?

- better resource management: YARN in Hadoop `efficiently and dynamically` allocates all cluster resources, resulting 
          in higher Hadoop utilization compared to previous versions which help in better cluster utilization.
- support different job mode: It supports `streaming, interactive and batch jobs`
- support many calculation frameworks: It supports MapReduce(hadoop), Spark, etc.

## Concepts

The fundamental idea of YARN is to split up the functionalities of `resource management` and `job scheduling/monitoring` 
into separate daemons. The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM). 
**An application is either a single job or a DAG of jobs**.

Yarn has four principal concepts:
- **ResourceManager**: The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system.
- **NodeManager**: The NodeManager is the per-machine framework agent who is responsible for containers, monitoring 
              their resource usage (cpu, memory, disk, network) and reporting the same to the ResourceManager/Scheduler.
- **ApplicationMaster**: Each application will have it's own dedicated ApplicationMaster. It's specific for each 
                  framework(MR, spark, etc.). It negotiates resources from the `ResourceManager` and working with 
                  the `NodeManager(s)` to execute and monitor the tasks.
- **Container**: It is a collection of physical resources such as RAM, CPU cores, and disks on a single node (like a vm in a hypervisor).
             It is supervised by the NodeManager and scheduled by the ResourceManager. Container can have different
             

### Resource manager

The ResourceManager has two main components: 
- **Scheduler**: The Scheduler is responsible for allocating resources to the various running applications subject to 
                 familiar constraints of capacities, queues etc. The Scheduler is pure scheduler in the sense that it 
                 performs no monitoring or tracking of status for the application. Also, it offers no guarantees about 
                 restarting failed tasks either due to application failure or hardware failures. The Scheduler 
                 performs its scheduling function based on the resource requirements of the applications; 
                 it does so based on the abstract notion of a resource Container which incorporates elements 
                 such as memory, cpu, disk, network etc. The Scheduler has a `pluggable policy` which is responsible 
                 for partitioning the cluster resources among the various queues, applications etc. The current 
                schedulers such as the `CapacityScheduler` and the `FairScheduler` would be some examples of plug-ins.

- **ApplicationsManager**: The ApplicationsManager is responsible for accepting `job-submissions`, negotiating the 
                 first container for executing the application specific `ApplicationMaster` and provides the service 
                 for restarting the ApplicationMaster container on failure. The per-application ApplicationMaster 
                 has the responsibility of negotiating appropriate resource containers from the Scheduler, tracking 
                 their status and monitoring for progress.

## 