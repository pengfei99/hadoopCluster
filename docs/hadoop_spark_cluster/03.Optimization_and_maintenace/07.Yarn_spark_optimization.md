# 

## hdfs optimization

```shell
dfs.namenode.edits.dir should be separate from the dfs.namenode.name.dir

# check fsimage config
```

## yarn optimization

The full yarn config(hadoop-3.3.6) options can be found [here](https://hadoop.apache.org/docs/r3.3.6/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)

### JVM Heap memory 

For the heap size, the default value is enough according to the ibm doc
https://www.ibm.com/docs/en/storage-scale-bda?topic=tuning-yarn


- Resource Manager Heap Size (resourcemanager_heapsize): default value is `1024Mb`, recommend value is `1024Mb`
- NodeManager Heap Size (nodemanager_heapsize): default value is `1024Mb`, recommend value is `1024Mb`

```shell
export YARN_NODEMANAGER_OPTS="-Xmx12288m -Xms12288m -Xmn4096m -Xss256K -XX:+DisableExplicitGC -XX:SurvivorRatio=8 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSClassUnloadingEnabled -XX:LargePageSizeInBytes=128M -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:-UseGCOverheadLimit  -Dcom.sun.management.jmxremote=false -Dcom.sun.management.jmxremote.port=13015 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false ${YARN_NODEMANAGER_OPTS}"
```

### Service thread count

We have identified the below service thread count for yarn.
- **yarn.resourcemanager.client.thread-count**: The number of threads used to handle applications manager requests. Default value is 50
- **yarn.resourcemanager.amlauncher.thread-count**: Number of threads used to launch/cleanup AM. Default value is 50
- **yarn.resourcemanager.resource-tracker.client.thread-count**: Number of threads to handle resource tracker calls. Default value is 50
- **yarn.resourcemanager.admin.client.thread-count**: Number of threads used to handle RM admin interface. Default value is 1

```xml
<property>
    <name>yarn.resourcemanager.amlauncher.thread-count</name>
    <value>50</value> <!-- Increase based on your load -->
</property>
<property>
    <name>yarn.resourcemanager.scheduler.client.thread-count</name>
    <value>50</value> <!-- Increase based on your load -->
</property>
<property>
    <name>yarn.resourcemanager.resource-tracker.client.thread-count</name>
    <value>50</value> <!-- Increase based on your load -->
</property>
```

### yarn-site.xml example


### Yarn Container virtual memory checking

Virtual memory checking is a mechanism in YARN where the NodeManager monitors the amount of virtual memory used by 
each container. If a container exceeds its allocated virtual memory limit, it is killed by the NodeManager.

`Virtual memory usage includes both physical memory (RAM) and swap space`. We can use below config options to
control the behavior:

- **yarn.nodemanager.pmem-check-enabled** :	Whether physical memory limits will be enforced for containers. Default value is `true`
- **yarn.nodemanager.vmem-check-enabled** : Whether virtual memory limits will be enforced for containers. Default value is `true`
- **yarn.nodemanager.vmem-pmem-ratio**:	Ratio between virtual memory to physical memory when setting memory limits for 
                  containers. Container allocations are expressed in terms of physical memory, and virtual memory usage 
                   is allowed to exceed this allocation by this ratio. Default value is `2.1`

Below is the recommended conf

```xml
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>

<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>true</value>
</property>
```

We disable virtual memory check to avoid Overly Strict Memory Enforcement:
- False Positives: Some applications may use a large amount of virtual memory without actually consuming an equivalent amount of physical memory. This can happen due to memory mapping or other factors that inflate virtual memory usage.
 Applications might get killed due to high virtual memory usage even though they are within the acceptable physical memory limits, leading to unnecessary application failures.
- Focus on Physical Memory Management:
- Reduce Resource Management Overhead: Disabling vmem checking can reduce the overhead on the NodeManager, making 
  resource management more efficient and reducing the likelihood of unnecessary container terminations.

## Spark on yarn optimization

### different steps during submit

1. Client->>ResourceManager: job submit request 
2. ResourceManager-->>NodeManager: resource negotiation
3. NodeManager-->>Executor: creation and initiation executor
4. Executor-->>Driver: register created executor to driver
5. Driver-->>Executor: driver send job to the registered executor
6. Executor-->>NodeManager: executor run the job and send result to the node manager
7. NodeManager-->>ResourceManager: demangler send result to resource manager
8. ResourceManager-->>Client: resource manager send result to spark client

### Add spark.yarn.jars

To make Spark runtime jars accessible from YARN side, you can specify spark.yarn.archive or spark.yarn.jars.
```shell

```

### Spark job conf

> To persist the spark submit conf, you can put them in to `spark-default.conf`

To optimize your spark on yarn job, you can try to modify the below config

```shell
1.spark.yarn.submit.file.replication 3 应用程序上传到HDFS的复制份数 2.spark.preserve.staging.files false 设置为true，在job结束后，将stage相关的文件保留而不是删除。 （一般无需保留，设置成false) 3.spark.yarn.scheduler.heartbeat.interal-ms 5000 Spark application master给YARN ResourceManager 发送心跳的时间间隔（ms） 4.spark.yarn.executor.memoryOverhead 1000 5.spark.serializer org.apache.spark.serializer.KryoSerializer 暂时只支持Java serializer和KryoSerializer序列化方式 6.spark.storage.memoryFraction 0.3 用来调整cache所占用的内存大小。默认为0.6。如果频繁发生Full GC，可以考虑降低这个比值，这样RDD Cache可用的内存空间减少（剩下的部分Cache数据就需要通过Disk Store写到磁盘上了），会带来一定的性能损失，但是腾出更多的内存空间用于执行任务，减少Full GC发生的次数，反而可能改善程序运行的整体性能。 7.spark.sql.shuffle.partitions 800 一个partition对应着一个task,如果数据量过大，可以调整次参数来减少每个task所需消耗的内存. 8.spark.sql.autoBroadcastJoinThreshold -1 当处理join查询时广播到每个worker的表的最大字节数，当设置为-1广播功能将失效。 9.spark.speculation false 如果设置成true，倘若有一个或多个task执行相当缓慢，就会被重启执行。（事实证明，这种做法会造成hdfs中临时文件的丢失，报找不到文件的错) 10.spark.sql.codegen true Spark SQL在每次执行次，先把SQL查询编译JAVA字节码。针对执行时间长的SQL查询或频繁执行的SQL查询，此配置能加快查询速度，因为它产生特殊的字节码去执行。但是针对很短的查询，可能会增加开销，因为它必须先编译每一个查询 11.spark.shuffle.spill false 如果设置成true，将会把spill的数据存入磁盘 12.spark.shuffle.consolidateFiles true 我们都知道shuffle默认情况下的文件数据为map tasks * reduce tasks,通过设置其为true,可以使spark合并shuffle的中间文件为reduce的tasks数目。 13.代码中 如果filter过滤后 会有很多空的任务或小文件产生，这时我们使用coalesce或repartition去减少RDD中partition数量。 RPC 服务线程调优 当 Spark 同时运行大量的 tasks 时，Driver 很容易出现 OOM，这是因为在 Driver 端的 Netty 服务器上产生大量 RPC 的请求积压，我们可以通过加大 RPC 服务的线程数解决 OOM 问题，比如 spark.rpc.io.serverThreads = 64。
```

spark.default.parallelism:
spark.sql.shuffle.partitions:

### Data locality
spark.locality.wait = 3s
spark.locality.wait.node = 5s
spark.locality.wait.rack = 10s

