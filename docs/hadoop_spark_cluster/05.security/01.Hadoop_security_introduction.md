
## 1. HDFS permission guide
The official site can be found [here](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html)

HDFS has a linux like POSIX ACLs, Each client process that accesses HDFS has a` two-part identity composed of the user name, and groups list`. 
Whenever HDFS must do a permissions check for a file or directory foo accessed by a client process,

- If the username matches the owner of foo, then the owner permissions are tested;
- Else if the group of foo matches any of member of the groups list, then the group permissions are tested;
- Otherwise, the other permissions of foo are tested.

### 1.1 User Identity
As of Hadoop 0.22, Hadoop supports two different modes of operation to determine the user’s identity, specified by the 
`hadoop.security.authentication` property:

- **simple**: In this mode of operation, the identity of a client process is determined by the host operating 
            system. On Unix-like systems, the user name is the equivalent of `whoami`.
- **kerberos**: In Kerberized operation, the identity of a client process is determined by its Kerberos credentials. 
           For example, in a Kerberized environment, a user may use the kinit utility to obtain a Kerberos 
           ticket-granting-ticket (TGT) and use klist to determine their current principal. When mapping a Kerberos 
           principal to an HDFS username, all components except for the primary are dropped. For example, a 
          principal todd/foobar@CORP.COMPANY.COM will act as the simple username todd on HDFS.

> Regardless of the mode of operation, the user identity mechanism is extrinsic to HDFS itself. There is no provision within HDFS for creating user identities, establishing groups, or processing user credentials.

### 1.2 simple authentication

```xml
<configuration>
<property>
	<name>hadoop.http.filter.initializers</name>
	<value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
	<description>A comma separated list of class names. Each class in the list must 
        extend org.apache.hadoop.http.FilterInitializer. The corresponding Filter will be initialized. 
        Then, the Filter will be applied to all user facing jsp and servlet web pages. The ordering of the list 
        defines the ordering of the filters.</description>
</property>

<property>
    <name>hadoop.http.authentication.type</name>
    <value>simple</value>
</property>

<property>
    <name>hadoop.http.authentication.signature.secret.file</name>
    <value>/data1/hadoop/hadoop/etc/hadoop/hadoop-http-auth-signature-secret</value>
  </property>

<property>
    <name>hadoop.http.authentication.simple.anonymous.allowed</name>
    <value>false</value>
</property>

<property>
    <name>hadoop.http.authentication.token.max-inactive-interval</name>
    <value>60</value>
</property>

</configuration>
```


## 2. Configure hdfs superuser group name

By default，only the user who start the namenode process has the super group privilege (e.g. hdfs by default).
But in practice, many commands(e.g. fsck) requires the super user privilege. To faiclitate this kind of actions,
we can set up a superuser group. All users in this group will have the super user privilege. So be careful when you
add user into this group
```xml
<property>
    <name>dfs.permissions.superusergroup</name>
    <value>hdfs</value>
    <description>The name of the group of super-users. The value should be a single group name. Default value is
    supergroup</description>
</property>
```